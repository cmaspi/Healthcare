{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = './Data/In-lab/'\n",
    "paths = sorted(list(os.listdir(parent_dir)))\n",
    "paths = np.array(paths)\n",
    "np.random.shuffle(paths)\n",
    "training_paths = paths[:-4]\n",
    "validation_paths = paths[-4:]\n",
    "interval_duration = 0.25\n",
    "vec_size = int(15_000 * interval_duration)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def return_dataset(paths):\n",
    "    data = []\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        temp = get_data(parent_dir+path, interval_duration=interval_duration)\n",
    "        dataX = temp[:, :vec_size].astype(float)    \n",
    "        remaining = temp[:, vec_size:]\n",
    "        # z score normalization\n",
    "        dataX -= dataX.mean()\n",
    "        dataX /= np.std(dataX)\n",
    "        temp = np.concatenate([dataX, remaining], axis=1)\n",
    "        data.append(temp)\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    dataX = data[:, :vec_size].astype(float)\n",
    "    dataY = data[:, -1].astype(float)\n",
    "    dataX, dataY = unison_shuffled_copies(dataX, dataY)\n",
    "    return dataX, dataY\n",
    "\n",
    "\n",
    "trainX, trainY = return_dataset(training_paths)\n",
    "valX, valY = return_dataset(validation_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainX = torch.Tensor(trainX, device=device)\n",
    "trainY = torch.Tensor(trainY, device=device)\n",
    "\n",
    "train_dataset = TensorDataset(trainX, trainY)\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "\n",
    "valX = torch.Tensor(valX, device=device)\n",
    "valY = torch.Tensor(valY, device=device)\n",
    "\n",
    "val_dataset = TensorDataset(valX, valY)\n",
    "val_dataloader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_model import TSNetwork\n",
    "\n",
    "model = TSNetwork()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss: {running_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
