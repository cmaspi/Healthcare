{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 17:28:37.456290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:38.953705947Z",
     "start_time": "2024-04-06T11:58:37.419786421Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "parent_dir = './Data/In-lab/'\n",
    "paths = sorted(list(os.listdir(parent_dir)))\n",
    "\n",
    "data = []\n",
    "interval_duration = 1\n",
    "vec_size = int(15_000 * interval_duration)\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    temp = get_data(parent_dir+path, interval_duration=interval_duration)\n",
    "    dataX = temp[:, :vec_size].astype(float)    \n",
    "    remaining = temp[:, vec_size:]\n",
    "    # # z score normalization\n",
    "    # dataX -= dataX.mean()\n",
    "    # dataX /= np.std(dataX)\n",
    "    # 0-1 normalization\n",
    "    dataX -= dataX.min()\n",
    "    dataX /= dataX.max()\n",
    "    temp = np.concatenate([dataX, remaining], axis=1)\n",
    "    data.append(temp)\n",
    "\n",
    "\n",
    "data = np.concatenate(data, axis=0)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "dataX, dataY = data[:,:vec_size].astype(float), data[:,-1].astype(float)\n",
    "trainX, valX, trainY, valY = train_test_split(dataX, dataY)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f10393a060be9931"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:04<00:00,  2.66it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "parent_dir = './Data/In-lab/'\n",
    "paths = sorted(list(os.listdir(parent_dir)))\n",
    "paths = np.array(paths)\n",
    "np.random.shuffle(paths)\n",
    "training_paths = paths[:-4]\n",
    "validation_paths = paths[-4:]\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def normalize_data(data, mode):\n",
    "    \"\"\"inplace operation\"\"\"\n",
    "    def minimum(data):\n",
    "        return min(min(data, key=lambda x: min(x)))\n",
    "    def maximum(data):\n",
    "        return max(max(data, key=lambda x: max(x)))\n",
    "\n",
    "    if mode == '0-1':\n",
    "        mini = minimum(data)\n",
    "        for subarr in data:\n",
    "            subarr -= mini\n",
    "        maxi = maximum(data)\n",
    "        for subarr in data:\n",
    "            subarr /= maxi\n",
    "    if mode == 'z':\n",
    "        raise NotImplementedError(\"not implemented yet\")\n",
    "\n",
    "\n",
    "def return_dataset(paths):\n",
    "    data = []\n",
    "    activities_list = []\n",
    "    labels_list = []\n",
    "    ema_list = []\n",
    "    for path in tqdm(paths):\n",
    "        dataX, ema, labels, activities = get_data_activity_chunks(parent_dir+path, sampling=5)\n",
    "        normalize_data(dataX, mode='0-1')\n",
    "        data.extend(dataX)\n",
    "        ema_list.append(ema)\n",
    "        labels_list.append(labels)\n",
    "        activities_list.append(activities)\n",
    "\n",
    "    return data, np.concatenate(ema_list, axis=0), np.concatenate(labels_list, axis=0), np.concatenate(activities_list, axis=0) \n",
    "\n",
    "\n",
    "trainX, trainEMA, trainY, trainActivities = return_dataset(training_paths)\n",
    "valX, valEMA, valY, valActivities = return_dataset(validation_paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:44.940493777Z",
     "start_time": "2024-04-06T11:58:38.959450223Z"
    }
   },
   "id": "d4674d676edb0190"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "14976\n",
      "14975\n",
      "11978\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(trainX[0]))\n",
    "print(len(trainX[1]))\n",
    "print(len(trainX[10]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:44.961322007Z",
     "start_time": "2024-04-06T11:58:44.941419831Z"
    }
   },
   "id": "fc44887f7074d8a6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(180, (14976,), (180,), (180, 14), (180,))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX), trainX[0].shape, trainY.shape, trainEMA.shape, trainActivities.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:44.994261025Z",
     "start_time": "2024-04-06T11:58:44.945936119Z"
    }
   },
   "id": "d8642c02d555bbe2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(np.arange(trainX[0, :1000].size), trainX[0, :1000])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb2359c2480384e5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from model import get_model\n",
    "import tensorflow.keras as keras\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:44.995395143Z",
     "start_time": "2024-04-06T11:58:44.964917758Z"
    }
   },
   "id": "350b26e00f155ef1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 17:28:44.978747: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 150)          91200       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 150, 1)      0           ['lstm[0][0]']                   \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 146, 4)       24          ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 142, 8)       168         ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 138, 16)      656         ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 134, 32)      2592        ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 130, 64)      10304       ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 126, 128)     41088       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 122, 256)     164096      ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['conv1d_6[0][0]']               \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 256)         0           ['conv1d_6[0][0]']               \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 512)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'global_max_pooling1d[0][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           32832       ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           1040        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4)            68          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            5           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 344,073\n",
      "Trainable params: 344,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(input_size=None)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:45.375730402Z",
     "start_time": "2024-04-06T11:58:44.969666835Z"
    }
   },
   "id": "e477f12efecac5ab"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            classes=np.unique(trainY),\n",
    "                                            y=trainY)\n",
    "weights = {\n",
    "    0: weights[0],\n",
    "    1: weights[1]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:45.376502452Z",
     "start_time": "2024-04-06T11:58:45.343438624Z"
    }
   },
   "id": "e5654c091b9a9f0a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=5, min_lr=1e-7, min_delta=1e-4)\n",
    "\n",
    "# history = model.fit(trainX,\n",
    "#                     trainY,\n",
    "#                     validation_data=(valX, valY),\n",
    "#                     verbose=1,\n",
    "#                     epochs=100,\n",
    "#                     class_weight=weights,\n",
    "#                     callbacks=[reduce_lr],\n",
    "#                     batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:45.428816816Z",
     "start_time": "2024-04-06T11:58:45.386184223Z"
    }
   },
   "id": "d5d9e5f7909e5d58"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(14975,)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:45.429297269Z",
     "start_time": "2024-04-06T11:58:45.386443272Z"
    }
   },
   "id": "db628462ede33942"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 (180,) 0.6\n"
     ]
    }
   ],
   "source": [
    "print(trainY.sum(), trainY.shape, trainY.sum()/trainY.size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T11:58:45.429522305Z",
     "start_time": "2024-04-06T11:58:45.386554393Z"
    }
   },
   "id": "f4b5447d88f993da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    overall_loss, overall_acc = [], []\n",
    "    for i in range(len(trainX)):\n",
    "        dX, dY = trainX[i].reshape((1,) + trainX[i].shape + (1,)), trainY[i:i+1]\n",
    "        loss, acc = model.train_on_batch(dX, dY, class_weight=weights)\n",
    "        overall_acc.append(acc)\n",
    "        overall_loss.append(loss)\n",
    "    print(f'accuracy: {np.mean(overall_acc)}, loss: {np.mean(overall_loss)}')\n",
    "    \n",
    "    val_overall_loss, val_overall_acc = [], []    \n",
    "    for i in range(len(valX)):\n",
    "        dX, dY = valX[i].reshape((1,) + valX[i].shape + (1,)), valY[i:i+1]\n",
    "        loss, acc = model.test_on_batch(dX, dY)\n",
    "        val_overall_acc.append(acc)\n",
    "        val_overall_loss.append(loss)\n",
    "    print(f'val accuracy: {np.mean(val_overall_acc)}, val loss: {np.mean(val_overall_loss)}')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-06T11:58:45.386643584Z"
    }
   },
   "id": "2abc4e70274069e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(len(valX)):\n",
    "    dX, dY = valX[i].reshape((1,) + valX[i].shape + (1,)), valY[i:i+1]\n",
    "    p = model.predict(dX)[0, 0]\n",
    "    pred.append(p)\n",
    "   \n",
    "    \n",
    "print(pred)\n",
    "print(valY)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4ea1b6a3ee1ad114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9866b820cd69a45d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
